# Deep Learning Speedruns (Timed Implementations)

This repository tracks my timed implementations of core deep learning architectures from scratch. 
Each architecture is implemented under strict time limits to simulate real research engineering conditions.

## Objectives
- Improve implementation speed
- Strengthen mathematical recall (RNN, LSTM, GRU, Attention)
- Build muscle memory for debugging shape issues
- Practice rapid prototyping for research workflows

## Time Limits
<!-- Place holder for Future -->
1. MLP: 
2. RNN: 
3. GRU: 
4. LSTM: 
5. Attention:
6. Transformer: 

## Structure
- Each folder contains:
  - Notebook implementation
  - Notes (bugs, reflections)
- The `failures` directory logs bugs and mistakes to analyze patterns.

## Current Progress
- [x] MLP
- [x] RNN
- [x] GRU
- [ ] LSTM
- [ ] Attention
- [ ] Transformer

## Future Additions
- Optimization tricks
- Quantization experiments
- Memory-efficient attention variants