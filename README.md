# Deep Learning Speedruns (Timed Implementations)

This repository tracks my timed implementations of core deep learning architectures from scratch.
Each architecture is implemented under strict time limits to simulate real research-engineering conditions.

## Objectives

* Improve implementation speed
* Strengthen mathematical recall (RNN, LSTM, GRU, Attention)
* Build muscle memory for debugging shape issues
* Practice rapid prototyping for research workflows

## Time Limits

<!-- Place holder for Future -->

1. MLP
2. RNN
3. GRU
4. LSTM
5. Attention
6. Transformer

## Structure

* Each folder contains:

  * Notebook implementation
  * Notes (bugs, reflections)
* The `failures` directory logs bugs and mistakes to analyze patterns.

## Current Progress

* [x] MLP
* [ ] RNN
* [ ] GRU
* [ ] LSTM
* [ ] Attention
* [ ] Transformer

### Live Implementations

For full real-time coding sessions, visit:
[YouTube](https://youtube.com/playlist?list=PLK2mmWCcKs6l1qGkvZegW6MOoBHey_ixg&si=5u1FCJXa9x2s3cvM)

## Future Additions

* Optimization tricks
* Quantization experiments
* Memory-efficient attention variants